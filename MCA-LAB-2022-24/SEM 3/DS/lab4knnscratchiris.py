# -*- coding: utf-8 -*-
"""LAB4knnscratchiris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pAbGs962smYNRUAeQGC31bG9JBcJ6nv_
"""



import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score

# Load the Iris dataset from scikit-learn
iris = datasets.load_iris()
data = pd.DataFrame(data=iris.data, columns=iris.feature_names)
target = iris.target

iris

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)

# Convert numerical labels to categorical labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Perform EDA
plt.figure(figsize=(12, 6))
for i in range(4):
    plt.subplot(2, 2, i + 1)
    plt.scatter(data.iloc[:, i], target)
    plt.xlabel(iris.feature_names[i])
    plt.ylabel("Target")
plt.show()

"""The scatter plots illustrate that petal length and petal width are the most effective features for distinguishing between Iris species, with Setosa clearly separated from Versicolor and Virginica. Sepal length and sepal width exhibit some separation but with noticeable overlap, making them less distinctive for species differentiation."""

# Implement kNN algorithm from scratch
def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))

def k_nearest_neighbors(X, y, query, k):
    distances = [(euclidean_distance(query, x), label) for x, label in zip(X, y)]
    sorted_distances = sorted(distances, key=lambda x: x[0])
    k_nearest = sorted_distances[:k]
    k_nearest_labels = [item[1] for item in k_nearest]
    return np.bincount(k_nearest_labels).argmax()

# Predict the labels for the test set
y_pred = [k_nearest_neighbors(X_train.to_numpy(), y_train, x, k) for x in X_test.to_numpy()]

# Calculate the accuracy of the kNN model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on the test set: {accuracy * 100:.2f}%")

"""we first calculate the predicted labels for the test set using the kNN algorithm implemented from scratch. Then, we use scikit-learn's accuracy_score to compute the accuracy of the model on the test data."""

# Ask the user for the value of 'k'
k = int(input("Enter the value of 'k' for the K-Nearest Neighbors algorithm: "))
# Now, you can take user input to create a new sample for prediction
user_input = input("Enter four feature values separated by commas (sepal length, sepal width, petal length, petal width): ")
user_values = [float(x) for x in user_input.split(',')]
new_instance = np.array([user_values])



# Predict the label for the new instance using the kNN algorithm
predicted_label = k_nearest_neighbors(X_train.to_numpy(), y_train, new_instance, k)
# Convert the predicted label back to the original class
predicted_class = label_encoder.inverse_transform([predicted_label])[0]
# Print the result

print()
print(f"The predicted class for the new instance is: {iris.target_names[predicted_class]}")

"""set the new_instance variable to the feature values of the new data you want to predict. Then, you use the k_nearest_neighbors function to predict the label for the new instance. Finally, convert the predicted label back to the original class name and print the result."""